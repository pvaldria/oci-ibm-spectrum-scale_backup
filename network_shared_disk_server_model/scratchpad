
less terraform.tfstate | grep -B30 shape | grep -v user_data | less


GUI install
if GUI count > 0 , then on GUI node run:
mmperfmon config generate --collectors=ss-mgmt-gui-1.privateb0.gpfs.oraclevcn.com, ss-mgmt-gui-2.privateb0.gpfs.oraclevcn.com
mmperfmon config show
mmchnode --perfmon -N all
mmlscluster
systemctl start gpfsgui
systemctl status gpfsgui


mmperfmon query compareNodes cpu
systemctl start gpfsgui
systemctl status gpfsgui


echo "get metrics mem_active, cpu_idle, gpfs_ns_read_ops, last 10 bucket_size 1" | /opt/IBM/zimon/zc 127.0.0.1

more /opt/IBM/zimon/ZIMonCollector.cfg

/usr/lpp/mmfs/gui/cli/lsnode

******** Post Deploy Step:  *************
Create GUI user after starting service.

Welcome to IBM Spectrum Scale
Before you get started with the IBM Spectrum Scale GUI, you need to create the first GUI user.
Switch to the command line interface on the GUI node and run the following command to create the first user:

/usr/lpp/mmfs/gui/cli/mkuser admin -g SecurityAdmin
Use admin001 as password.
Welcome to IBM Spectrum Scale
Before you get started with the IBM Spectrum Scale GUI, you need to create the first GUI user.
Switch to the command line interface on the GUI node and run the following command to create the first user:

One of them is "Master GUI Node" , other is just "GUI Node"
[root@ss-mgmt-gui-1 ~]# /usr/lpp/mmfs/gui/cli/lsnode
Hostname                                   IP        Description     Role       Product version Connection status GPFS status Last updated
ss-ces-nic1-1.privateb0.gpfs.oraclevcn.com 10.0.6.6                  ces        5.0.3.2         HEALTHY           HEALTHY     10/5/19 6:08 PM
ss-ces-nic1-2.privateb0.gpfs.oraclevcn.com 10.0.6.7                  ces        5.0.3.2         HEALTHY           HEALTHY     10/5/19 6:08 PM
ss-mgmt-gui-1.privateb0.gpfs.oraclevcn.com 10.0.6.11 Master GUI Node management 5.0.3.2         HEALTHY           HEALTHY     10/5/19 6:07 PM
ss-server-1.privateb0.gpfs.oraclevcn.com   10.0.6.4                  storage    5.0.3.2         HEALTHY           HEALTHY     10/5/19 6:07 PM
ss-server-2.privateb0.gpfs.oraclevcn.com   10.0.6.5                  storage    5.0.3.2         HEALTHY           HEALTHY     10/5/19 6:07 PM
EFSSG1000I The command completed successfully.
[root@ss-mgmt-gui-1 ~]#

[root@ss-mgmt-gui-1 ~]# ssh ss-mgmt-gui-2
Last login: Sat Oct  5 18:32:48 2019 from ss-server-1.privateb0.gpfs.oraclevcn.com
[root@ss-mgmt-gui-2 ~]# /usr/lpp/mmfs/gui/cli/lsnode
Hostname                                   IP        Description Role       Product version Connection status GPFS status Last updated
ss-ces-nic1-1.privateb0.gpfs.oraclevcn.com 10.0.6.6              ces        5.0.3.2         HEALTHY           HEALTHY     10/5/19 6:08 PM
ss-ces-nic1-2.privateb0.gpfs.oraclevcn.com 10.0.6.7              ces        5.0.3.2         HEALTHY           HEALTHY     10/5/19 6:08 PM
ss-mgmt-gui-2.privateb0.gpfs.oraclevcn.com 10.0.6.10 GUI Node    management 5.0.3.2         HEALTHY           HEALTHY     10/5/19 6:07 PM
ss-server-1.privateb0.gpfs.oraclevcn.com   10.0.6.4              storage    5.0.3.2         HEALTHY           HEALTHY     10/5/19 6:07 PM
ss-server-2.privateb0.gpfs.oraclevcn.com   10.0.6.5              storage    5.0.3.2         HEALTHY           HEALTHY     10/5/19 6:07 PM
EFSSG1000I The command completed successfully.
[root@ss-mgmt-gui-2 ~]#




********************************************

Below is the instruction to deploy the protocol node - I will not show you the protocol user authentication for CIFS because it require the Active Directory and the AD has to have RFC2307 schema implemented.

Below steps need to be inserted just before configure the Zimon sensors:

Install additional dependencies for the protocol.
[root@prot01 ~]# yum install nfs-utils bind-utils  (for Active Directory integration)
[root@prot01 ~]# yum install nfs-utils openldap-client sssd-common sssd-ldap   (for LDAP integration)
Create the CES shared root directory.
[root@prot01 ~]# mmchconfig cesSharedRoot=/gpfs/fs1/cesfs
Modify the cluster configuration for protocol service.
[root@prot01 ~]# mmchconfig profile=gpfsProtocolDefaults
Enable CES on protocol nodes.
[root@prot01 ~]# mmchnode --ces-enable -N prot01,prot02
Change the file system ACL semantic, and remount the filesystem in order to reflect the change.
[root@prot01 ~]# mmchfs fs1 -k nfs4
[root@prot01 ~]# mmumount fs1 -a
[root@prot01 ~]# mmmount fs1 -a
Startup the protocol nodes.
[root@prot01 ~]# mmstartup -N cesNodes
Add the virtual IP addresses to the CES IP address pool.
[root@prot01 ~]# mmces address add --ces-ip 10.0.7.101,10.0.7.102
Enable NFS/SMB services.
[root@prot01 ~]# mmces service enable NFS
[root@prot01 ~]# mmces service enable SMB
Configure the user-defined authentication for the protocol service.
[root@prot01 ~]# mmuserauth service create --data-access-method file --type userdefined
Create the CES export.
[root@scproto5011 ~]# mmcrfileset fs1 gpfsHome
[root@scproto5011 ~]# mmlinkfileset fs1 gpfsHome -J /gpfs/fs1/home



#  availability_domain = "${lookup(data.oci_identity_availability_domains.ADs.availability_domains[var.availability_domain - 1],"name")}"

# availability_domain = "${lookup(data.oci_identity_availability_domains.ADs.availability_domains[((count.index % 2 == 0) ? local.site1 : local.site2)],"name")}"
#  availability_domain = "${lookup(data.oci_identity_availability_domains.ADs.availability_domains[((floor(count.index / var.nsd_nodes_per_pool) % 2 == 0) ? local.site1 : local.site2)],"name")}"


# [((floor(count.index / var.block_volumes_per_pool) % 2 == 0) ? local.site1 : local.site2)],"name")}"

/*
  metadata = {
    ssh_authorized_keys = "${var.ssh_public_key}"
    user_data = "${base64encode(data.template_file.boot_script.rendered)}"
  }
*/

#"privateBSubnetsFQDN=\"(local.dual_nics ? ${oci_core_virtual_network.gpfs.dns_label}.oraclevcn.com ${oci_core_subnet.privateb.*.dns_label[0]}.${oci_core_virtual_network.gpfs.dns_label}.oraclevcn.com)\"",

#  availability_domain = "${lookup(data.oci_identity_availability_domains.ADs.availability_domains[var.availability_domain - 1],"name")}"
# latest is below
#availability_domain = "${lookup(data.oci_identity_availability_domains.ADs.availability_domains[((count.index % 2 == 0) ? local.site1 : local.site2)],"name")}"


/*
data "template_file" "boot_script" {
  template =  "${file("${var.scripts_directory}/boot.sh.tpl")}"
  vars = {
    version = var.spectrum_scale["version"]
    downloadUrl = var.spectrum_scale["download_url"]
    sshPrivateKey = var.ssh_private_key
    sshPublicKey = var.ssh_public_key
    totalNsdNodePools = var.total_nsd_node_pools
    nsdNodesPerPool = var.nsd_nodes_per_pool
    nsdNodeCount = (var.total_nsd_node_pools * var.nsd_nodes_per_pool)
    nsdNodeHostnamePrefix = var.nsd_node["hostname_prefix"]
    clientNodeCount = var.client_node["node_count"]
    clientNodeHostnamePrefix = var.client_node["hostname_prefix"]
    blockSize=var.spectrum_scale["block_size"]
    dataReplica=var.spectrum_scale["data_replica"]
    metadataReplica=var.spectrum_scale["metadata_replica"]
    gpfsMountPoint=var.spectrum_scale["gpfs_mount_point"]
    sharedDataDiskCount=(var.total_nsd_node_pools * var.block_volumes_per_pool)
    installerNode = "${var.nsd_node["hostname_prefix"]}${var.installer_node}"
    privateSubnetsFQDN = "${oci_core_virtual_network.gpfs.dns_label}.oraclevcn.com ${oci_core_subnet.private.*.dns_label[0]}.${oci_core_virtual_network.gpfs.dns_label}.oraclevcn.com"
    privateBSubnetsFQDN = "${oci_core_virtual_network.gpfs.dns_label}.oraclevcn.com ${oci_core_subnet.privateb.*.dns_label[0]}.${oci_core_virtual_network.gpfs.dns_label}.oraclevcn.com"
    companyName=var.callhome["company_name"]
    companyID=var.callhome["company_id"]
    countryCode=var.callhome["country_code"]
    emailaddress=var.callhome["emailaddress"]
  }
}
*/



/*
 "oci compute volume-attachment attach --type iscsi --is-shareable true  --instance-id  ${oci_core_instance.nsd_node[(b+(a*var.nsd_nodes_per_pool))].id}  --volume-id ${oci_core_volume.shared_data_block_volume[(var.nsd_nodes_per_pool_list*(c+(a*var.block_volumes_per_pool)))].id} --device ${var.volume_attach_device_mapping[(c)]}  --config-file ~/.oci/config ;sleep 81s; "
*/



#  availability_domain = "${lookup(data.oci_identity_availability_domains.ADs.availability_domains[var.availability_domain - 1],"name")}"
# last used is below
# availability_domain = "${lookup(data.oci_identity_availability_domains.ADs.availability_domains[((count.index % 2 == 0) ? local.site1 : local.site2)],"name")}"
# availability_domains[((floor(count.index / var.block_volumes_per_pool) % 2 == 0) ? local.site1 : local.site2)],"name")}"

# search oci_core_virtual_network.gpfs.dns_label.oraclevcn.com oci_core_subnet.privateb.*.dns_label[0].oci_core_virtual_network.gpfs.dns_label.oraclevcn.com gpfs.oraclevcn.com private0.gpfs.oraclevcn.com


# Recommended for SAS Grid Shared File System - use 3 nsd_nodes_per_pool and 30 block_volumes_per_pool for max throughput
#variable "total_nsd_node_pools" { default="1" }
#variable "nsd_nodes_per_pool" { default="3" }
#variable "block_volumes_per_pool" { default="30" }

##command="mmcrcluster -N node.stanza -r /usr/bin/ssh -R /usr/bin/scp -C ss-demo01.privateb2.ibmssvcnv3.oraclevcn.com -A"
##`$command`
##while [ $? -ne 0 ]; do
##echo "retry..."
##sleep 10s;
##`$command`
##done;


# below not working, hence consolidating all nsd stanza files
# Create a file system fs1 using the first NSD stanza.
# Add additional NSDs to the existing filesystem fs1
###for poolIndex in `seq 1 $totalNsdNodePools`;
###do
###if [ $poolIndex -eq 1 ]; then
###echo "mmcrfs fs1  -F /tmp/nsd.stanza.sv${nsdNodesPerPool}.set${poolIndex} -B $blockSize -m 2 -M 2 -r 2 -R 2"
###mmcrfs fs1  -F /tmp/nsd.stanza.sv${nsdNodesPerPool}.set${poolIndex} -B $blockSize -m 2 -M 2 -r 2 -R 2
###sleep 120s
###else
###echo "mmadddisk -F /tmp/nsd.stanza.sv${nsdNodesPerPool}.set${poolIndex}  "
###mmadddisk -F /tmp/nsd.stanza.sv${nsdNodesPerPool}.set${poolIndex}
###sleep 60s
###fi
###done



#KERNALVERSION=`uname -a  | gawk -F" " '{ print $3 }' | sed "s|.x86_64||g"`
#sudo yum install kernel-devel-$KERNALVERSION  -y
#yum install kernel-headers-$KERNALVERSION -y
#yum install gcc gcc-c++ -y


